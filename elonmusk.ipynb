{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=C:\\key.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=C:\\key.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pycontractions  import Contractions\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from google.cloud import translate_v2 as translate\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT\n",
    "# import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google client object\n",
    "client = translate.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2668 entries, 0 to 2667\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Tweets          2668 non-null   object        \n",
      " 1   Retweets        2668 non-null   int64         \n",
      " 2   Likes           2668 non-null   int64         \n",
      " 3   Date            2668 non-null   datetime64[ns]\n",
      " 4   Cleaned_Tweets  2668 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 104.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Read the data --> encoding utf-8, some characters were not coming correctly which utf-8 solved the issue\n",
    "df = pd.read_csv('./data/cleandata.csv', parse_dates=['Date'],encoding = \"utf-8\") \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique values for column Tweets is 2642\n",
      "\n",
      "\n",
      "The number of unique values for column Retweets is 1834\n",
      "The min value for column Retweets is 41\n",
      "The max value for column Retweets is 681707\n",
      "\n",
      "The number of unique values for column Likes is 2598\n",
      "The min value for column Likes is 933\n",
      "The max value for column Likes is 4780787\n",
      "\n",
      "The number of unique values for column Date is 2668\n",
      "The min value for column Date is 2022-01-27 21:00:09\n",
      "The max value for column Date is 2022-10-27 16:17:39\n",
      "\n",
      "The number of unique values for column Cleaned_Tweets is 2382\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_basic_info(dataframe):\n",
    "    cols_list = dataframe.columns.tolist()\n",
    "    data_types = df.dtypes    \n",
    "    \n",
    "    for col in cols_list:        \n",
    "        # get the number of unique entries for each column\n",
    "        no_unique_value = dataframe[col].nunique()\n",
    "        print(f'The number of unique values for column {col} is {no_unique_value}')\n",
    "\n",
    "        # Check for the data type and get the min and max value\n",
    "        if data_types[col] != 'object':\n",
    "            min_value = dataframe[col].min()\n",
    "            max_value = dataframe[col].max()\n",
    "            print(f'The min value for column {col} is {min_value}')\n",
    "            print(f'The max value for column {col} is {max_value}\\n')\n",
    "        else:\n",
    "            print('\\n')  \n",
    "get_basic_info(df)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = Contractions('./ignore/GoogleNews-vectors-negative300.bin')\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Replace the non-ASCII characters\n",
    "df['Cleaned_Tweets'] = df['Cleaned_Tweets'].replace({r'[^\\x00-\\x7F]+':''}, regex=True)\n",
    "\n",
    "# stopwords list\n",
    "stop = stopwords.words('english')\n",
    "stop += ['im','ie','ete', 'dont', 'cant', 'would','wont','doesnt','must','might','also','almost','so', 'haha']\n",
    "\n",
    "def clean(text):\n",
    "    # Remove spaces from beginning and ending of the text\n",
    "    text = text.strip()\n",
    "    # Fix quotes\n",
    "    text = text.replace(\"‚Äô\", \"'\") \\\n",
    "        .replace(\"‚Äò\", \"'\") \\\n",
    "        .replace(\"‚Äù\", '\"') \\\n",
    "        .replace(\"‚Äú\", '\"')\n",
    "\n",
    "    # Replace &amp; with and\n",
    "    text = text.replace('&amp;','and')\n",
    "\n",
    "    text = text.replace('&gt;', 'greater than ')\n",
    "\n",
    "    text = text.replace('&lt;', 'less than ')\n",
    "\n",
    "    text = text.replace('-', '')\n",
    "    \n",
    "    # Fix contractions\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "   \n",
    "    #val = re.fullmatch(r'[\\w\\s]*((?<=\\d)[\\.,\\/:%]|(?=\\d))*[\\w\\s]*',text, flags=re.MULTILINE)\n",
    "    # Replace only the punctuations from the text --> do not remove decimals or , or % from digits\n",
    "    text = re.sub(r'[^\\w\\s%](?!\\d)', ' ', text, flags=re.MULTILINE)  \n",
    "\n",
    "    return text\n",
    "\n",
    "df['Cleaned_Tweets'] = df['Cleaned_Tweets'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2668, 5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1996, 6)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the tokens of the tweets\n",
    "df['Token_Counts'] = df['Cleaned_Tweets'].apply(lambda x: len(x.split(' ')))\n",
    "# Remove tweets with less than 3 tokens\n",
    "df = df[df['Token_Counts'] > 3].reset_index(drop=True).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_language'] = df['Cleaned_Tweets'].apply(lambda x : client.detect_language(x)['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Date</th>\n",
       "      <th>Cleaned_Tweets</th>\n",
       "      <th>Token_Counts</th>\n",
       "      <th>tweet_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Vox Populi Vox Dei</td>\n",
       "      <td>5709</td>\n",
       "      <td>53880</td>\n",
       "      <td>2022-10-19 16:59:23</td>\n",
       "      <td>Voice of the people voice of God</td>\n",
       "      <td>4</td>\n",
       "      <td>la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Baltasar Graci√°n, Or√°culo Manual y Arte de Pru...</td>\n",
       "      <td>2903</td>\n",
       "      <td>36061</td>\n",
       "      <td>2022-09-19 19:33:52</td>\n",
       "      <td>Baltasar Gracin Oracle Manual and Art of Prudence</td>\n",
       "      <td>9</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Standup is my side-hustle</td>\n",
       "      <td>8866</td>\n",
       "      <td>155973</td>\n",
       "      <td>2022-08-17 04:59:51</td>\n",
       "      <td>Standup is my side hustle</td>\n",
       "      <td>4</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>Schadenfreude oder Schatzifreude?</td>\n",
       "      <td>1227</td>\n",
       "      <td>29721</td>\n",
       "      <td>2022-07-24 03:54:01</td>\n",
       "      <td>Schadenfreude or Schatzifreude</td>\n",
       "      <td>4</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>@BillyM2k üêÅ + ‚å®Ô∏è v1.05</td>\n",
       "      <td>290</td>\n",
       "      <td>5508</td>\n",
       "      <td>2022-07-11 21:25:28</td>\n",
       "      <td>v105</td>\n",
       "      <td>4</td>\n",
       "      <td>ku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>A veritable ‚Äúsock aficionado‚Äù</td>\n",
       "      <td>2974</td>\n",
       "      <td>56477</td>\n",
       "      <td>2022-07-05 17:44:09</td>\n",
       "      <td>A real sock amateur</td>\n",
       "      <td>4</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>Happy July 4th! https://t.co/KYN2XO712Z</td>\n",
       "      <td>16091</td>\n",
       "      <td>289550</td>\n",
       "      <td>2022-07-04 20:14:41</td>\n",
       "      <td>Happy July 4th</td>\n",
       "      <td>4</td>\n",
       "      <td>te-Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>Con te ‚Ä¶ partir√≤</td>\n",
       "      <td>5404</td>\n",
       "      <td>88159</td>\n",
       "      <td>2022-05-29 20:38:24</td>\n",
       "      <td>I39ll leave with you</td>\n",
       "      <td>4</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>Per aspera ad astra!</td>\n",
       "      <td>41296</td>\n",
       "      <td>518423</td>\n",
       "      <td>2022-04-26 20:15:32</td>\n",
       "      <td>Through difficulties to the stars</td>\n",
       "      <td>5</td>\n",
       "      <td>la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>üöÄüí´‚ô•Ô∏è Yesss!!! ‚ô•Ô∏èüí´üöÄ https://t.co/0T9HzUHuh6</td>\n",
       "      <td>348158</td>\n",
       "      <td>2608578</td>\n",
       "      <td>2022-04-25 19:43:22</td>\n",
       "      <td>Yessss</td>\n",
       "      <td>4</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets  Retweets    Likes  \\\n",
       "67                                   Vox Populi Vox Dei      5709    53880   \n",
       "353   Baltasar Graci√°n, Or√°culo Manual y Arte de Pru...      2903    36061   \n",
       "534                           Standup is my side-hustle      8866   155973   \n",
       "668                   Schadenfreude oder Schatzifreude?      1227    29721   \n",
       "751                              @BillyM2k üêÅ + ‚å®Ô∏è v1.05       290     5508   \n",
       "798                       A veritable ‚Äúsock aficionado‚Äù      2974    56477   \n",
       "810             Happy July 4th! https://t.co/KYN2XO712Z     16091   289550   \n",
       "1075                                   Con te ‚Ä¶ partir√≤      5404    88159   \n",
       "1466                               Per aspera ad astra!     41296   518423   \n",
       "1471         üöÄüí´‚ô•Ô∏è Yesss!!! ‚ô•Ô∏èüí´üöÄ https://t.co/0T9HzUHuh6    348158  2608578   \n",
       "\n",
       "                    Date                                     Cleaned_Tweets  \\\n",
       "67   2022-10-19 16:59:23                   Voice of the people voice of God   \n",
       "353  2022-09-19 19:33:52  Baltasar Gracin Oracle Manual and Art of Prudence   \n",
       "534  2022-08-17 04:59:51                          Standup is my side hustle   \n",
       "668  2022-07-24 03:54:01                     Schadenfreude or Schatzifreude   \n",
       "751  2022-07-11 21:25:28                                               v105   \n",
       "798  2022-07-05 17:44:09                                A real sock amateur   \n",
       "810  2022-07-04 20:14:41                                     Happy July 4th   \n",
       "1075 2022-05-29 20:38:24                               I39ll leave with you   \n",
       "1466 2022-04-26 20:15:32                  Through difficulties to the stars   \n",
       "1471 2022-04-25 19:43:22                                             Yessss   \n",
       "\n",
       "      Token_Counts tweet_language  \n",
       "67               4             la  \n",
       "353              9             es  \n",
       "534              4             da  \n",
       "668              4             de  \n",
       "751              4             ku  \n",
       "798              4             es  \n",
       "810              4        te-Latn  \n",
       "1075             4             es  \n",
       "1466             5             la  \n",
       "1471             4             ms  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate the text with different language to english --> tweets that were not recognized in english will be translated to english\n",
    "df_temp = df[df['tweet_language'] != 'en'].copy()\n",
    "df_temp['Cleaned_Tweets'] = df_temp['Cleaned_Tweets'].apply(lambda x : client.translate(x, target_language='en')['translatedText'])\n",
    "# Replace the punctuations in the translated text\n",
    "df_temp['Cleaned_Tweets'] = df_temp['Cleaned_Tweets'].replace({r'[^\\w\\s]':''}, regex=True)\n",
    "df_temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67                       Voice of the people voice of God\n",
       "353     Baltasar Gracin Oracle Manual and Art of Prudence\n",
       "534                             Standup is my side hustle\n",
       "668                        Schadenfreude or Schatzifreude\n",
       "751                                                  v105\n",
       "798                                   A real sock amateur\n",
       "810                                        Happy July 4th\n",
       "1075                                 I39ll leave with you\n",
       "1466                    Through difficulties to the stars\n",
       "1471                                               Yessss\n",
       "1688                                    Thank you Germany\n",
       "1739                                  I love the mariachi\n",
       "1782    I would like to thank you very much The future...\n",
       "Name: Cleaned_Tweets, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the index of the df_temp and update df at the same index for the cleaned tweets\n",
    "df.loc[df_temp.index, 'Cleaned_Tweets'] = df_temp['Cleaned_Tweets']\n",
    "\n",
    "# Verify if data got updated\n",
    "df.loc[df_temp.index, 'Cleaned_Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1075,'Cleaned_Tweets'] = 'I will leave with you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1991, 7)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because of the different language the tokens were not calculated properly\n",
    "# Count the tokens of the tweets\n",
    "df['Token_Counts'] = df['Cleaned_Tweets'].apply(lambda x: len(x.split(' ')))\n",
    "# Remove tweets with less than 3 tokens\n",
    "df = df[df['Token_Counts'] > 3].reset_index(drop=True).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./ignore/data_after_translation.csv',encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra list into stop words identified from tweets\n",
    "stop += [\"cb\", \"twh\", \"og\", \"onto\", \"tf\", \"oge\", \"fyi\", \"v\", \"um\", \"lb\", \"g\", \"bros\", \"cc\", \"mgmt\", \"vw\", \"aka\",\n",
    "         \"tsla\", \"n\", \"%\", \"f\",\"thy\", \"thee\", \"bi\", \"r\", \"mr\", \"vu\", \"dj\", \"ci\", \"h\", \"con\", \"bf\", \"lmk\", \"incl\",\n",
    "         \"uh\", \"ii\", \"tbc\", \"mf\", \"ye\", \"ya\", \"eg\", \"hi\", \"wow\", \"v2\", \"bro\", \"went\", \"oh\", \"bs\", \"none\", \"das\",\n",
    "         \"guy\", \"yup\", \"took\", \"saw\", \"obv\", \"got\", \"un\", \"kind\", \"gave\", \"gone\", \"id\", \"btw\", \"thank\", \"due\",\n",
    "         \"tbh\", \"c\", \"keep\", \"able\", \"around\", \"vs\", \"per\", \"yet\", \"imo\", \"x\", \"soon\", \"away\", \"coming\", \"sure\",\n",
    "         \"take\", \"go\", \"ok\", \"please\", \"something\", \"going\", \"making\", \"cannot\", \"want\", \"done\", \"let\", \"use\", \"say\",\n",
    "         \"made\", \"see\", \"back\", \"possible\", \"could\", \"us\", \"need\", \"yeah\", \"know\", \"get\", \"yes\", \"right\", \"still\",\n",
    "         \"think\", \"make\", \"like\", \"many\", \"much\", \"people\"]\n",
    "tmp_stop = []\n",
    "\n",
    "for val in stop:\n",
    "    tmp_stop.append(val.capitalize())\n",
    "    tmp_stop.append(val.upper())\n",
    "\n",
    "stop += tmp_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stop words\n",
    "def process_text(text):    \n",
    "    text = \" \".join([word for word in text.split() if word not in stop])\n",
    "    return text\n",
    "\n",
    "df['Cleaned_Tweets'] = df['Cleaned_Tweets'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1453, 7)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the tokens of the tweets\n",
    "df['Token_Counts'] = df['Cleaned_Tweets'].apply(lambda x: len(x.split(' ')))\n",
    "# Remove tweets with less than 3 tokens\n",
    "df = df[df['Token_Counts'] > 3].reset_index(drop=True).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model pipeline\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Emanuel/bertweet-emotion-base\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Emanuel/bertweet-emotion-base\"\n",
    ")\n",
    "device = -1 #torch.cuda.current_device() if torch.cuda.is_available else -1\n",
    "model_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'label': 'joy', 'score': 0.9908571839332581}...\n",
       "1    [{'label': 'joy', 'score': 0.6109302639961243}...\n",
       "2    [{'label': 'sadness', 'score': 0.4522185921669...\n",
       "3    [{'label': 'joy', 'score': 0.696447491645813},...\n",
       "4    [{'label': 'anger', 'score': 0.632644712924957...\n",
       "Name: Emotion_Scores, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion_Scores'] = model_pipeline(df['Cleaned_Tweets'].to_list(), top_k=None)\n",
    "df['Emotion_Scores'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        joy\n",
       "1        joy\n",
       "2    sadness\n",
       "3        joy\n",
       "4      anger\n",
       "Name: Emotion1, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign top 1 emotion\n",
    "df['Emotion1'] = df['Emotion_Scores'].apply(lambda x: x[0]['label'])\n",
    "df['Emotion1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "# Extract the noun keyphrases, so create the vectorizer with the pattern and pass it to KeyBert\n",
    "vectorizer = KeyphraseCountVectorizer(pos_pattern='<NNP.*>+')\n",
    "df['Noun_Keyphrases_Score'] = kw_model.extract_keywords(docs=df['Cleaned_Tweets'].to_list(),\\\n",
    "                         vectorizer=vectorizer, stop_words='english', top_n=5)\n",
    "df['Noun_Keyphrases'] = df['Noun_Keyphrases_Score'].apply(lambda record: [x[0] for x in record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter', 0.6036), ('lot', 0.3114), ('cool', 0.293)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Noun_Keyphrases_Score'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [twitter, lot, cool]\n",
       "1       [twitter hq, twitter]\n",
       "2                   [twitter]\n",
       "3            [twitter, thing]\n",
       "4                       [fan]\n",
       "                ...          \n",
       "1448                   [time]\n",
       "1449              [manganese]\n",
       "1450              [manganese]\n",
       "1451                       []\n",
       "1452                       []\n",
       "Name: Noun_Keyphrases, Length: 1453, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Noun_Keyphrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting score types for serialization\n",
    "def fix_float_type(input):\n",
    "    return [(x[0], str(x[1])) for x in input]\n",
    "df['Noun_Keyphrases_Score'] = df['Noun_Keyphrases_Score'].apply(fix_float_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-09 18:15:14,044 loading file C:\\Users\\baira\\.flair\\models\\pos-english\\a9a73f6cd878edce8a0fa518db76f441f1cc49c2525b2b4557af278ec2f0659e.121306ea62993d04cd1978398b68396931a39eb47754c8a06a87f325ea70ac63\n",
      "2022-12-09 18:15:14,855 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    }
   ],
   "source": [
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n",
    "\n",
    "def flair_pos_tagging(sentence):\n",
    "    # print(sentence)\n",
    "    verbs = set()\n",
    "    adjectives = set()\n",
    "    sen = Sentence(sentence)\n",
    "    tagger.predict(sen)\n",
    "\n",
    "    for label in sen.get_labels('pos'):\n",
    "        \n",
    "        if label.value[0:2] == 'VB' and label.score > 0.75:\n",
    "            verbs.add(label.data_point.text)\n",
    "            # print(verbs)\n",
    "        if label.value[0:2] == 'JJ' and label.score > 0.75:\n",
    "            adjectives.add(label.data_point.text)\n",
    "            # print(adjectives)\n",
    "\n",
    "    return list(verbs), list(adjectives)\n",
    "\n",
    "df['verbs'], df['adjectives'] = zip(*df['Cleaned_Tweets'].str.lower()\\\n",
    "                                            .apply(flair_pos_tagging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_to_remove = ['get','are','is','am','have','has','been','seen','had','do','took','be',\n",
    "                    'make','does','like','did','see','was','go','got','get','want','getting','gets', 'exist',\n",
    "                    'done','doing','went','uses','says','known','let','given' ,'gave','makes','goes',\n",
    "                    'gone','going','saw','being','were']\n",
    "\n",
    "def remove_words(row):\n",
    "    verbs_list = []\n",
    "\n",
    "    if len(row) > 0:\n",
    "        for i in row:\n",
    "            if i not in verbs_to_remove:\n",
    "                verbs_list.append(i)\n",
    "    return verbs_list                  \n",
    "\n",
    "df['verbs'] = df['verbs'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_to_remove = ['many','most','much','such']\n",
    "\n",
    "def remove_words(row):\n",
    "    adj_list = []\n",
    "\n",
    "    if len(row) > 0:\n",
    "        for i in row:\n",
    "            if i not in adj_to_remove:\n",
    "                adj_list.append(i)\n",
    "    return adj_list                  \n",
    "\n",
    "df['adjectives'] = df['adjectives'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('./data/processed_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rough Work --> code that can be used in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "# Adding spaces after the removal of punctuations, as there might not be space \n",
    "# added in the text after the use of punctuation\n",
    "# no_spaces = len(string.punctuation)\n",
    "    \n",
    "# text = text.translate(str.maketrans(string.punctuation,' ' * no_spaces))  \n",
    "\n",
    "# # Remove stopwords\n",
    "# text = \" \".join([word for word in text.split() if word not in stop])\n",
    "\n",
    "# with open(\"./data/cleandata.csv\", 'rb') as rawdata:\n",
    "#     result = chardet.detect(rawdata.read())\n",
    "\n",
    "# # check what the character encoding might be\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d33e8667d8178efa7deef3a321041744c51a72df3bf15f8600944ed1b5e39ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
